#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä»£ç ç›®çš„ï¼šå¯¹tifæ–‡ä»¶è¿›è¡Œé«˜æ•ˆé•¶åµŒåˆå¹¶ï¼ˆæŒ‰åŒºåŸŸå’Œå¹´ä»½åˆ†ç»„ï¼‰
åŠŸèƒ½ï¼š
1. è¯»å–è£å‰ªåçš„æ‰€æœ‰tifæ–‡ä»¶
2. æŒ‰åŒºåŸŸå’Œå¹´ä»½åˆ†ç»„è¿›è¡Œé•¶åµŒ
3. ä½¿ç”¨åˆ†å±‚é•¶åµŒç­–ç•¥æé«˜å¤„ç†æ•ˆç‡
4. æ”¯æŒå¤§æ–‡ä»¶å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
5. è¾“å‡ºæŒ‰åŒºåŸŸå¹´ä»½å‘½åçš„é•¶åµŒç»“æœ
6. å¯é€‰æ‹©ä½¿ç”¨VRTè™šæ‹Ÿæ …æ ¼æŠ€æœ¯åŠ é€Ÿ
7. è‡ªåŠ¨ä¸ºè¾“å‡ºæ–‡ä»¶æ·»åŠ é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”

é•¶åµŒç­–ç•¥åˆ†æï¼š
- VRTæŠ€æœ¯ - è™šæ‹Ÿé•¶åµŒï¼Œé€Ÿåº¦æœ€å¿«ï¼Œå†…å­˜æ¶ˆè€—æœ€å°
- ç‰¹åˆ«é€‚åˆå«å¤§é‡nodataå€¼çš„æ …æ ¼æ•°æ®
- å……åˆ†åˆ©ç”¨å¤šæ ¸CPUï¼Œé¿å…64GBå†…å­˜é™åˆ¶

ä½œè€…ï¼šé”å¤šå® (ruiduobao)
æ—¥æœŸï¼š2025å¹´1æœˆ
"""

import os
import sys
import logging
import time
import re
import shutil
from datetime import datetime
from pathlib import Path
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
def setup_logging(output_dir):
    """
    è®¾ç½®æ—¥å¿—è®°å½•
    """
    log_filename = f"esri_mosaic_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    log_path = os.path.join(output_dir, log_filename)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return log_path

def create_color_table():
    """
    åˆ›å»ºé¢œè‰²æ˜ å°„è¡¨
    
    Returns:
        gdal.ColorTable: GDALé¢œè‰²è¡¨å¯¹è±¡
    """
    try:
        from osgeo import gdal
        
        # åˆ›å»ºé¢œè‰²è¡¨å¯¹è±¡
        color_table = gdal.ColorTable()
        
        # å®šä¹‰é¢œè‰²æ˜ å°„ (R, G, B, Alpha)
        # 0: ç©ºå€¼/æ— æ•°æ® - é€æ˜
        color_table.SetColorEntry(0, (0, 0, 0, 0))
        
        # 1: äººå·¥æ— - è“è‰² (RGB: 30, 144, 255)
        color_table.SetColorEntry(1, (30, 144, 255, 255))
        
        # 2: å…¶ä»–æ¤è¢« - ç»¿è‰² (RGB: 34, 139, 34) 
        color_table.SetColorEntry(2, (34, 139, 34, 255))
        
        # 3: éæ¤è¢« - æµ…ç°è‰² (RGB: 192, 192, 192)
        color_table.SetColorEntry(3, (192, 192, 192, 255))
        
        return color_table
        
    except ImportError:
        logging.error("GDALæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºé¢œè‰²æ˜ å°„è¡¨")
        return None

def build_overviews(file_path, overview_levels=None):
    """
    ä¸ºæ …æ ¼æ–‡ä»¶æ„å»ºå¤–ç½®é‡‘å­—å¡”ï¼ˆ.ovræ–‡ä»¶ï¼‰
    
    Args:
        file_path (str): æ …æ ¼æ–‡ä»¶è·¯å¾„
        overview_levels (list): é‡‘å­—å¡”å±‚çº§åˆ—è¡¨ï¼Œé»˜è®¤ä¸º[2, 4, 8, 16, 32, 64]
    
    Returns:
        bool: æ„å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        from osgeo import gdal
        
        logging.info(f"å¼€å§‹ä¸ºæ–‡ä»¶æ„å»ºé‡‘å­—å¡”: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        # è®¾ç½®é»˜è®¤é‡‘å­—å¡”å±‚çº§
        if overview_levels is None:
            overview_levels = [2, 4, 8, 16, 32, 64]
        
        logging.info(f"é‡‘å­—å¡”å±‚çº§: {overview_levels}")
        
        # æ‰“å¼€æ•°æ®é›†
        dataset = gdal.Open(file_path, gdal.GA_ReadOnly)
        if dataset is None:
            logging.error(f"æ— æ³•æ‰“å¼€æ–‡ä»¶: {file_path}")
            return False
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        width = dataset.RasterXSize
        height = dataset.RasterYSize
        bands = dataset.RasterCount
        
        logging.info(f"æ•°æ®é›†ä¿¡æ¯: {width}x{height} åƒç´ , {bands} ä¸ªæ³¢æ®µ")
        
        # è®¾ç½®é‡‘å­—å¡”æ„å»ºé€‰é¡¹
        resampling_method = "NEAREST"  # å¯¹äºåˆ†ç±»æ•°æ®ä½¿ç”¨æœ€è¿‘é‚»
        
        logging.info(f"ä½¿ç”¨é‡é‡‡æ ·æ–¹æ³•: {resampling_method}")
        
        # è®¾ç½®é‡‘å­—å¡”å‹ç¼©é€‰é¡¹
        logging.info("è®¾ç½®é‡‘å­—å¡”å‹ç¼©é€‰é¡¹: LZW")
        gdal.SetConfigOption('COMPRESS_OVERVIEW', 'LZW')
        gdal.SetConfigOption('TILED_OVERVIEW', 'YES')  # å¯ç”¨åˆ†å—å­˜å‚¨
        gdal.SetConfigOption('BIGTIFF_OVERVIEW', 'IF_SAFER')  # å¤§æ–‡ä»¶æ”¯æŒ
        
        # æ„å»ºé‡‘å­—å¡”
        logging.info("æ­£åœ¨æ„å»ºå‹ç¼©é‡‘å­—å¡”...")
        
        result = dataset.BuildOverviews(resampling_method, overview_levels)
        
        if result == 0:
            logging.info("é‡‘å­—å¡”æ„å»ºæˆåŠŸ!")
            
            # æ£€æŸ¥ç”Ÿæˆçš„.ovræ–‡ä»¶
            ovr_file = file_path + ".ovr"
            if os.path.exists(ovr_file):
                ovr_size = os.path.getsize(ovr_file) / (1024 * 1024)  # MB
                logging.info(f"é‡‘å­—å¡”æ–‡ä»¶: {ovr_file}")
                logging.info(f"é‡‘å­—å¡”æ–‡ä»¶å¤§å°: {ovr_size:.2f} MB")
            else:
                logging.warning("æœªæ‰¾åˆ°.ovræ–‡ä»¶ï¼Œé‡‘å­—å¡”å¯èƒ½å†…åµŒåœ¨åŸæ–‡ä»¶ä¸­")
            
            # è·å–é‡‘å­—å¡”ä¿¡æ¯
            band = dataset.GetRasterBand(1)
            overview_count = band.GetOverviewCount()
            logging.info(f"é‡‘å­—å¡”å±‚æ•°: {overview_count}")
            
            # æ˜¾ç¤ºæ¯å±‚é‡‘å­—å¡”çš„å°ºå¯¸
            for i in range(overview_count):
                overview = band.GetOverview(i)
                ov_width = overview.XSize
                ov_height = overview.YSize
                scale_factor = width / ov_width
                logging.info(f"  å±‚çº§ {i+1}: {ov_width}x{ov_height} (ç¼©æ”¾æ¯”ä¾‹: 1:{scale_factor:.0f})")
            
            dataset = None  # å…³é—­æ•°æ®é›†
            return True
            
        else:
            logging.error(f"é‡‘å­—å¡”æ„å»ºå¤±è´¥ï¼Œé”™è¯¯ä»£ç : {result}")
            dataset = None
            return False
            
    except ImportError:
        logging.error("GDALæœªå®‰è£…ï¼Œæ— æ³•æ„å»ºé‡‘å­—å¡”")
        return False
    except Exception as e:
        logging.error(f"æ„å»ºé‡‘å­—å¡”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logging.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False
    
    finally:
        # æ¸…ç†GDALé…ç½®é€‰é¡¹
        try:
            from osgeo import gdal
            gdal.SetConfigOption('COMPRESS_OVERVIEW', None)
            gdal.SetConfigOption('TILED_OVERVIEW', None)
            gdal.SetConfigOption('BIGTIFF_OVERVIEW', None)
        except:
            pass
        
        # ç¡®ä¿æ•°æ®é›†è¢«æ­£ç¡®å…³é—­
        try:
            if 'dataset' in locals() and dataset is not None:
                dataset = None
        except:
            pass

def apply_color_table_and_build_pyramids(file_path):
    """
    ä¸ºæ …æ ¼æ–‡ä»¶åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å¹¶æ„å»ºé‡‘å­—å¡”
    
    Args:
        file_path (str): æ …æ ¼æ–‡ä»¶è·¯å¾„
    
    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    try:
        from osgeo import gdal, gdalconst
        
        logging.info(f"å¼€å§‹ä¸ºæ–‡ä»¶åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        # æ‰“å¼€æ•°æ®é›†ï¼ˆå¯å†™æ¨¡å¼ï¼‰
        dataset = gdal.Open(file_path, gdal.GA_Update)
        if dataset is None:
            logging.error(f"æ— æ³•æ‰“å¼€æ–‡ä»¶: {file_path}")
            return False
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„è¡¨
        color_table = create_color_table()
        if color_table is None:
            logging.error("æ— æ³•åˆ›å»ºé¢œè‰²æ˜ å°„è¡¨")
            dataset = None
            return False
        
        # åº”ç”¨é¢œè‰²æ˜ å°„è¡¨åˆ°ç¬¬ä¸€ä¸ªæ³¢æ®µ
        band = dataset.GetRasterBand(1)
        band.SetColorInterpretation(gdalconst.GCI_PaletteIndex)
        result = band.SetColorTable(color_table)
        
        if result == 0:
            logging.info("é¢œè‰²æ˜ å°„è¡¨åº”ç”¨æˆåŠŸ")
        else:
            logging.warning(f"é¢œè‰²æ˜ å°„è¡¨åº”ç”¨è¿”å›ä»£ç : {result}")
        
        # å¼ºåˆ¶å†™å…¥ç£ç›˜
        dataset.FlushCache()
        dataset = None  # å…³é—­æ•°æ®é›†
        
        # æ„å»ºé‡‘å­—å¡”
        pyramid_success = build_overviews(file_path)
        
        if pyramid_success:
            logging.info("é¢œè‰²æ˜ å°„è¡¨å’Œé‡‘å­—å¡”å¤„ç†å®Œæˆ")
            return True
        else:
            logging.warning("é¢œè‰²æ˜ å°„è¡¨åº”ç”¨æˆåŠŸï¼Œä½†é‡‘å­—å¡”æ„å»ºå¤±è´¥")
            return True  # é¢œè‰²æ˜ å°„è¡¨æˆåŠŸå°±ç®—æˆåŠŸ
            
    except ImportError:
        logging.error("GDALæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†é¢œè‰²æ˜ å°„è¡¨å’Œé‡‘å­—å¡”")
        return False
    except Exception as e:
        logging.error(f"åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”æ—¶å‡ºé”™: {str(e)}")
        return False
    
    finally:
        # ç¡®ä¿æ•°æ®é›†è¢«æ­£ç¡®å…³é—­
        try:
            if 'dataset' in locals() and dataset is not None:
                dataset = None
        except:
            pass

def force_remove_directory(dir_path, max_retries=3):
    """
    å¼ºåˆ¶åˆ é™¤ç›®å½•åŠå…¶æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬éç©ºç›®å½•ï¼‰
    
    å‚æ•°:
        dir_path: è¦åˆ é™¤çš„ç›®å½•è·¯å¾„
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
        bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
    """
    if not os.path.exists(dir_path):
        logging.info(f"ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤: {dir_path}")
        return True
    
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨shutil.rmtreeå¼ºåˆ¶åˆ é™¤æ•´ä¸ªç›®å½•æ ‘
            shutil.rmtree(dir_path, ignore_errors=False)
            logging.info(f"ä¸´æ—¶ç›®å½•åˆ é™¤æˆåŠŸ: {dir_path}")
            return True
            
        except PermissionError as e:
            logging.warning(f"åˆ é™¤ç›®å½•æƒé™é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                continue
            else:
                logging.error(f"åˆ é™¤ç›®å½•å¤±è´¥ï¼Œæƒé™ä¸è¶³: {dir_path}")
                return False
                
        except OSError as e:
            logging.warning(f"åˆ é™¤ç›®å½•ç³»ç»Ÿé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                continue
            else:
                logging.error(f"åˆ é™¤ç›®å½•å¤±è´¥ï¼Œç³»ç»Ÿé”™è¯¯: {dir_path}")
                return False
                
        except Exception as e:
            logging.error(f"åˆ é™¤ç›®å½•æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            return False
    
    return False

def parse_filename(filename):
    """
    è§£ææ–‡ä»¶åï¼Œæå–åŒºåŸŸå’Œå¹´ä»½ä¿¡æ¯
    
    å‚æ•°:
        filename: æ–‡ä»¶åï¼Œå¦‚ "zone10_classification_2017_rf100-0000000000-0000000000.tif"
    
    è¿”å›:
        tuple: (zone, year) æˆ– (None, None) å¦‚æœè§£æå¤±è´¥
    """
    try:
        # åŒ¹é…æ–‡ä»¶åæ¨¡å¼ï¼šzone{æ•°å­—}_classification_{å¹´ä»½}_...
        pattern = r'zone(\d+)_classification_(\d{4})_'
        match = re.search(pattern, filename)
        
        if match:
            zone = f"zone{match.group(1)}"
            year = match.group(2)
            return zone, year
        else:
            logging.warning(f"æ— æ³•è§£ææ–‡ä»¶å: {filename}")
            return None, None
            
    except Exception as e:
        logging.error(f"è§£ææ–‡ä»¶åæ—¶å‡ºé”™ {filename}: {str(e)}")
        return None, None

def group_files_by_zone_year(input_dir):
    """
    æŒ‰åŒºåŸŸå’Œå¹´ä»½å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç»„
    
    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½•
    
    è¿”å›:
        dict: {(zone, year): [file_paths]}
    """
    grouped_files = defaultdict(list)
    
    try:
        # è·å–æ‰€æœ‰tifæ–‡ä»¶
        for file in os.listdir(input_dir):
            if file.lower().endswith('.tif'):
                file_path = os.path.join(input_dir, file)
                zone, year = parse_filename(file)
                
                if zone and year:
                    grouped_files[(zone, year)].append(file_path)
                    logging.debug(f"æ–‡ä»¶ {file} å½’ç±»åˆ° {zone}_{year}")
                else:
                    logging.warning(f"è·³è¿‡æ— æ³•è§£æçš„æ–‡ä»¶: {file}")
        
        # è¾“å‡ºåˆ†ç»„ç»Ÿè®¡
        logging.info(f"æ–‡ä»¶åˆ†ç»„å®Œæˆï¼Œå…±æ‰¾åˆ° {len(grouped_files)} ä¸ªåŒºåŸŸ-å¹´ä»½ç»„åˆ:")
        for (zone, year), files in grouped_files.items():
            logging.info(f"  {zone}_{year}: {len(files)} ä¸ªæ–‡ä»¶")
        
        return dict(grouped_files)
        
    except Exception as e:
        logging.error(f"æ–‡ä»¶åˆ†ç»„æ—¶å‡ºé”™: {str(e)}")
        return {}

# VRTç›¸å…³å‡½æ•°ä¿ç•™ï¼Œå…¶ä»–å‡½æ•°å·²ç§»é™¤ä»¥ä¼˜åŒ–æ€§èƒ½

def create_unified_mosaic(input_files, output_path):
    """
    ä½¿ç”¨gdalwarpåˆ›å»ºç»Ÿä¸€æŠ•å½±çš„é•¶åµŒæ–‡ä»¶ï¼ˆè§£å†³æŠ•å½±ä¸ä¸€è‡´é—®é¢˜ï¼‰
    
    å‚æ•°:
        input_files: è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
    
    è¿”å›:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        from osgeo import gdal
        
        logging.info(f"ä½¿ç”¨gdalwarpåˆ›å»ºç»Ÿä¸€æŠ•å½±é•¶åµŒï¼ŒåŒ…å« {len(input_files)} ä¸ªæ–‡ä»¶")
        logging.info("ç»Ÿä¸€æŠ•å½±åˆ°WGS84åœ°ç†åæ ‡ç³»ï¼Œè§£å†³UTMæŠ•å½±ä¸ä¸€è‡´é—®é¢˜")
        logging.info("è¾“å‡ºæ•°æ®ç±»å‹: uint8 (Byte)ï¼Œåƒç´ å€¼èŒƒå›´: 0-255")
        logging.info("NoDataå€¼è®¾ç½®ä¸º: 0 (è¡¨ç¤ºç©ºå€¼/æ— æ•°æ®åŒºåŸŸ)")
        
        # è®¾ç½®GDALé…ç½®ä»¥ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨ï¼ˆé«˜æ€§èƒ½é…ç½®ï¼‰
        gdal.SetConfigOption('GDAL_CACHEMAX', '4096')  # 4GBç¼“å­˜ï¼ˆæå‡ï¼‰
        gdal.SetConfigOption('GDAL_NUM_THREADS', 'ALL_CPUS')   # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        gdal.SetConfigOption('VSI_CACHE', 'TRUE')  # å¯ç”¨VSIç¼“å­˜
        gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN', 'EMPTY_DIR')  # ä¼˜åŒ–æ–‡ä»¶æ‰“å¼€
        gdal.SetConfigOption('GDAL_MAX_DATASET_POOL_SIZE', '200')  # å¢åŠ æ•°æ®é›†æ± å¤§å°
        gdal.SetConfigOption('GDAL_SWATH_SIZE', '0')  # ç¦ç”¨swathé™åˆ¶ï¼Œæå‡å¤§æ–‡ä»¶å¤„ç†
        gdal.SetConfigOption('GDAL_MAX_RAW_BLOCK_CACHE_SIZE', '200000000')  # 200MBåŸå§‹å—ç¼“å­˜
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ä½¿ç”¨gdalwarpè¿›è¡Œé‡æŠ•å½±å’Œé•¶åµŒ
        warp_options = gdal.WarpOptions(
            format='GTiff',
            dstSRS='EPSG:4326',  # ç›®æ ‡æŠ•å½±ï¼šWGS84åœ°ç†åæ ‡ç³»
            xRes=0.0001,  # çº¦10ç±³åˆ†è¾¨ç‡ï¼ˆåœ°ç†åæ ‡ï¼‰
            yRes=0.0001,
            resampleAlg='nearest',  # æœ€å¿«çš„é‡é‡‡æ ·æ–¹æ³•
            outputType=gdal.GDT_Byte,  # è¾“å‡ºæ•°æ®ç±»å‹ä¸ºuint8ï¼ˆByteï¼‰
            srcNodata=0,  # æºæ•°æ®nodataå€¼
            dstNodata=0,  # ç›®æ ‡nodataå€¼
            creationOptions=[
                'COMPRESS=LZW',  # LZWå‹ç¼©
                'TILED=YES',  # ç“¦ç‰‡å­˜å‚¨
                'BLOCKXSIZE=1024',
                'BLOCKYSIZE=1024',
                'NUM_THREADS=ALL_CPUS',  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
                'BIGTIFF=IF_SAFER',
                'SPARSE_OK=TRUE'
            ],
            multithread=True,  # å¯ç”¨å¤šçº¿ç¨‹
            warpMemoryLimit=2048,  # 2GBå†…å­˜é™åˆ¶ï¼ˆæå‡ï¼‰
            callback=gdal.TermProgress_nocb  # æ˜¾ç¤ºè¿›åº¦
        )
        
        logging.info("å¼€å§‹é‡æŠ•å½±å’Œé•¶åµŒå¤„ç†...")
        
        # æ·»åŠ è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œè¿›åº¦ç›‘æ§
        try:
            # å¯ç”¨GDALå¼‚å¸¸å¤„ç†
            gdal.UseExceptions()
            
            ds = gdal.Warp(output_path, input_files, options=warp_options)
            if ds is None:
                logging.error("gdalwarpé•¶åµŒå¤±è´¥ï¼šè¿”å›ç©ºæ•°æ®é›†")
                return False
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦çœŸæ­£åˆ›å»º
            if not os.path.exists(output_path):
                logging.error(f"è¾“å‡ºæ–‡ä»¶æœªåˆ›å»º: {output_path}")
                return False
                
        except Exception as e:
            logging.error(f"gdalwarpå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logging.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            return False
        finally:
            # æ¢å¤GDALé»˜è®¤é”™è¯¯å¤„ç†
            gdal.DontUseExceptions()
        

        
        # è·å–è¾“å‡ºä¿¡æ¯
        logging.info(f"é•¶åµŒç»“æœå°ºå¯¸: {ds.RasterXSize} x {ds.RasterYSize}")
        logging.info(f"æ³¢æ®µæ•°: {ds.RasterCount}")
        
        # è·å–åœ°ç†èŒƒå›´ä¿¡æ¯
        geotransform = ds.GetGeoTransform()
        if geotransform:
            min_x = geotransform[0]
            max_y = geotransform[3]
            max_x = min_x + geotransform[1] * ds.RasterXSize
            min_y = max_y + geotransform[5] * ds.RasterYSize
            logging.info(f"åœ°ç†èŒƒå›´: X({min_x:.6f}, {max_x:.6f}), Y({min_y:.6f}, {max_y:.6f})")
        
        # è·å–è¾“å‡ºæ–‡ä»¶å¤§å°
        ds = None  # å…³é—­æ–‡ä»¶
        file_size_gb = os.path.getsize(output_path) / (1024 * 1024 * 1024)
        logging.info(f"è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size_gb:.2f} GB")
        logging.info(f"é•¶åµŒåˆ›å»ºæˆåŠŸ: {output_path}")
        
        # åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”
        logging.info("å¼€å§‹ä¸ºè¾“å‡ºæ–‡ä»¶åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”...")
        color_pyramid_success = apply_color_table_and_build_pyramids(output_path)
        
        if color_pyramid_success:
            logging.info("é¢œè‰²æ˜ å°„è¡¨å’Œé‡‘å­—å¡”å¤„ç†å®Œæˆ")
        else:
            logging.warning("é¢œè‰²æ˜ å°„è¡¨å’Œé‡‘å­—å¡”å¤„ç†å¤±è´¥ï¼Œä½†ä¸»æ–‡ä»¶é•¶åµŒæˆåŠŸ")
        
        return True
        
    except ImportError:
        logging.error("GDALæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨é•¶åµŒåŠŸèƒ½")
        return False
    except Exception as e:
        logging.error(f"åˆ›å»ºé•¶åµŒæ—¶å‡ºé”™: {str(e)}")
        return False

def create_vrt_mosaic(input_files, vrt_path):
    """
    åˆ›å»ºVRTè™šæ‹Ÿé•¶åµŒæ–‡ä»¶ï¼ˆæœ€å¿«çš„é•¶åµŒæ–¹æ³•ï¼‰
    
    å‚æ•°:
        input_files: è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        vrt_path: VRTè¾“å‡ºè·¯å¾„
    
    è¿”å›:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        from osgeo import gdal
        
        logging.info(f"åˆ›å»ºVRTè™šæ‹Ÿé•¶åµŒï¼ŒåŒ…å« {len(input_files)} ä¸ªæ–‡ä»¶")
        logging.info("VRTæŠ€æœ¯ï¼šè™šæ‹Ÿé•¶åµŒï¼Œé€Ÿåº¦æœ€å¿«ï¼Œå†…å­˜æ¶ˆè€—æœ€å°")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(vrt_path), exist_ok=True)
        
        # åˆ›å»ºVRTé€‰é¡¹
        vrt_options = gdal.BuildVRTOptions(
            resolution='highest',  # ä½¿ç”¨æœ€é«˜åˆ†è¾¨ç‡
            outputSRS='EPSG:4326',  # è¾“å‡ºæŠ•å½±
            srcNodata=0,  # æºæ•°æ®nodataå€¼
            VRTNodata=0,  # VRT nodataå€¼
            separate=False,  # ä¸åˆ†ç¦»æ³¢æ®µ
            allowProjectionDifference=True  # å…è®¸æŠ•å½±å·®å¼‚
        )
        
        # åˆ›å»ºVRT
        logging.info("å¼€å§‹åˆ›å»ºVRTæ–‡ä»¶...")
        
        try:
            # å¯ç”¨GDALå¼‚å¸¸å¤„ç†
            gdal.UseExceptions()
            
            vrt_ds = gdal.BuildVRT(vrt_path, input_files, options=vrt_options)
            if vrt_ds is None:
                logging.error("VRTåˆ›å»ºå¤±è´¥ï¼šè¿”å›ç©ºæ•°æ®é›†")
                return False
            
            # æ£€æŸ¥VRTæ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if not os.path.exists(vrt_path):
                logging.error(f"VRTæ–‡ä»¶æœªåˆ›å»º: {vrt_path}")
                return False
                
        except Exception as e:
            logging.error(f"VRTåˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logging.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            return False
        finally:
            # æ¢å¤GDALé»˜è®¤é”™è¯¯å¤„ç†
            gdal.DontUseExceptions()
        
        # è·å–VRTä¿¡æ¯
        logging.info(f"VRTå°ºå¯¸: {vrt_ds.RasterXSize} x {vrt_ds.RasterYSize}")
        logging.info(f"æ³¢æ®µæ•°: {vrt_ds.RasterCount}")
        
        vrt_ds = None  # å…³é—­VRT
        logging.info(f"VRTåˆ›å»ºæˆåŠŸ: {vrt_path}")
        return True
        
    except ImportError:
        logging.error("GDALæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨VRTåŠŸèƒ½")
        return False
    except Exception as e:
        logging.error(f"åˆ›å»ºVRTæ—¶å‡ºé”™: {str(e)}")
        return False

def convert_vrt_to_tiff(vrt_path, output_path):
    """
    å°†VRTè½¬æ¢ä¸ºå®é™…çš„TIFFæ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹ä¼˜åŒ–ï¼Œå†…å­˜å‹å¥½ï¼‰
    
    å‚æ•°:
        vrt_path: VRTæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºTIFFè·¯å¾„
    
    è¿”å›:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        from osgeo import gdal
        
        logging.info(f"å°†VRTè½¬æ¢ä¸ºTIFF: {output_path}")
        logging.info("ä½¿ç”¨å¤šçº¿ç¨‹è½¬æ¢ï¼Œå……åˆ†åˆ©ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼ˆé«˜æ€§èƒ½é…ç½®ï¼‰")
        logging.info("è¾“å‡ºæ•°æ®ç±»å‹: uint8 (Byte)ï¼Œåƒç´ å€¼èŒƒå›´: 0-255")
        logging.info("NoDataå€¼è®¾ç½®ä¸º: 0 (è¡¨ç¤ºç©ºå€¼/æ— æ•°æ®åŒºåŸŸ)")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # è½¬æ¢é€‰é¡¹ï¼Œä¸“é—¨ä¼˜åŒ–å¤§é‡nodataçš„å¤„ç†
        translate_options = gdal.TranslateOptions(
            format='GTiff',
            creationOptions=[
                'COMPRESS=LZW',  # LZWå‹ç¼©ï¼Œå¯¹nodataåŒºåŸŸå‹ç¼©æ•ˆæœå¥½
                'TILED=YES',  # ç“¦ç‰‡å­˜å‚¨ï¼Œæé«˜è®¿é—®é€Ÿåº¦
                'BLOCKXSIZE=1024',  # å¢å¤§å—å¤§å°ï¼Œå‡å°‘I/Oæ¬¡æ•°
                'BLOCKYSIZE=1024',
                'NUM_THREADS=ALL_CPUS',  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
                'BIGTIFF=IF_SAFER',  # å¤§æ–‡ä»¶è‡ªåŠ¨ä½¿ç”¨BigTIFF
                'SPARSE_OK=TRUE'  # ç¨€ç–æ–‡ä»¶ä¼˜åŒ–ï¼Œå¯¹å¤§é‡nodataæœ‰æ•ˆ
            ],
            outputType=gdal.GDT_Byte,  # è¾“å‡ºæ•°æ®ç±»å‹ä¸ºuint8ï¼ˆByteï¼‰  
            noData=0,  # nodataå€¼
            stats=False,  # è·³è¿‡ç»Ÿè®¡è®¡ç®—ä»¥æé«˜é€Ÿåº¦
            callback=gdal.TermProgress_nocb  # æ˜¾ç¤ºè¿›åº¦ä½†ä¸å›è°ƒ
        )
        
        # æ‰§è¡Œè½¬æ¢
        logging.info("å¼€å§‹VRTåˆ°TIFFè½¬æ¢...")
        
        # æ·»åŠ è¯¦ç»†çš„é”™è¯¯å¤„ç†
        try:
            # å¯ç”¨GDALå¼‚å¸¸å¤„ç†
            gdal.UseExceptions()
            
            ds = gdal.Translate(output_path, vrt_path, options=translate_options)
            if ds is None:
                logging.error("VRTè½¬æ¢å¤±è´¥ï¼šè¿”å›ç©ºæ•°æ®é›†")
                return False
                
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦çœŸæ­£åˆ›å»º
            if not os.path.exists(output_path):
                logging.error(f"VRTè½¬æ¢è¾“å‡ºæ–‡ä»¶æœªåˆ›å»º: {output_path}")
                return False
                
        except Exception as e:
            logging.error(f"VRTè½¬æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logging.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            return False
        finally:
            # æ¢å¤GDALé»˜è®¤é”™è¯¯å¤„ç†
            gdal.DontUseExceptions()
        
        # ä¿ç•™é¢œè‰²æ˜ å°„è¡¨
        try:
            vrt_ds = gdal.Open(vrt_path)
            if vrt_ds is not None:
                vrt_band = vrt_ds.GetRasterBand(1)
                color_table = vrt_band.GetColorTable()
                if color_table is not None:
                    output_band = ds.GetRasterBand(1)
                    output_band.SetColorTable(color_table)
                    logging.info("å·²ä¿ç•™é¢œè‰²æ˜ å°„è¡¨åˆ°è¾“å‡ºæ–‡ä»¶")
                vrt_ds = None
        except Exception as e:
            logging.warning(f"ä¿ç•™é¢œè‰²æ˜ å°„è¡¨åˆ°è¾“å‡ºæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        # è·³è¿‡ç»Ÿè®¡è®¡ç®—ä»¥æé«˜å¤„ç†é€Ÿåº¦
        logging.info("å·²è·³è¿‡æ …æ ¼ç»Ÿè®¡è®¡ç®—ä»¥ä¼˜åŒ–æ€§èƒ½")
        
        # è·å–è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        file_size_gb = os.path.getsize(output_path) / (1024 * 1024 * 1024)
        logging.info(f"è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size_gb:.2f} GB")
        
        ds = None  # å…³é—­æ–‡ä»¶
        logging.info(f"VRTè½¬æ¢å®Œæˆ: {output_path}")
        
        # åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”
        logging.info("å¼€å§‹ä¸ºè¾“å‡ºæ–‡ä»¶åº”ç”¨é¢œè‰²æ˜ å°„è¡¨å’Œæ„å»ºé‡‘å­—å¡”...")
        color_pyramid_success = apply_color_table_and_build_pyramids(output_path)
        
        if color_pyramid_success:
            logging.info("é¢œè‰²æ˜ å°„è¡¨å’Œé‡‘å­—å¡”å¤„ç†å®Œæˆ")
        else:
            logging.warning("é¢œè‰²æ˜ å°„è¡¨å’Œé‡‘å­—å¡”å¤„ç†å¤±è´¥ï¼Œä½†ä¸»æ–‡ä»¶è½¬æ¢æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logging.error(f"VRTè½¬æ¢æ—¶å‡ºé”™: {str(e)}")
        return False

def fast_vrt_mosaic(input_dir, output_path):
    """
    æœ€å¿«çš„VRTé•¶åµŒæ–¹æ³•ï¼ˆä¸“ä¸º25æ ¸CPUå’Œ64GBå†…å­˜ä¼˜åŒ–ï¼‰
    
    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½•
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–æ‰€æœ‰tifæ–‡ä»¶
        tif_files = []
        for file in os.listdir(input_dir):
            if file.lower().endswith('.tif'):
                file_path = os.path.join(input_dir, file)
                tif_files.append(file_path)
        
        if not tif_files:
            logging.error("æœªæ‰¾åˆ°ä»»ä½•tifæ–‡ä»¶")
            return False
        
        logging.info(f"æ‰¾åˆ° {len(tif_files)} ä¸ªtifæ–‡ä»¶")
        
        # è®¡ç®—æ€»æ–‡ä»¶å¤§å°
        total_size_gb = sum(os.path.getsize(f) for f in tif_files) / (1024**3)
        logging.info(f"æ€»æ•°æ®å¤§å°: {total_size_gb:.2f} GB")
        
        # ä½¿ç”¨VRTæŠ€æœ¯ï¼ˆæœ€å¿«æ–¹æ³•ï¼‰
        temp_vrt = output_path.replace('.tif', '_temp.vrt')
        
        logging.info("=== å¼€å§‹VRTé•¶åµŒå¤„ç† ===")
        if create_vrt_mosaic(tif_files, temp_vrt):
            # å°†VRTè½¬æ¢ä¸ºTIFF
            success = convert_vrt_to_tiff(temp_vrt, output_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_vrt):
                os.remove(temp_vrt)
                logging.info("æ¸…ç†ä¸´æ—¶VRTæ–‡ä»¶å®Œæˆ")
            
            return success
        else:
            logging.error("VRTé•¶åµŒå¤±è´¥")
            return False
        
    except Exception as e:
        logging.error(f"VRTé•¶åµŒæ—¶å‡ºé”™: {str(e)}")
        return False

def create_batch_mosaic(input_files, output_path, batch_size=20):
    """
    åˆ†æ‰¹å¤„ç†é•¶åµŒï¼Œé¿å…å†…å­˜æº¢å‡ºï¼ˆä¸“ä¸ºå¤§é‡æ–‡ä»¶ä¼˜åŒ–ï¼‰
    
    å‚æ•°:
        input_files: è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
        batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡ä»¶æ•°é‡
    
    è¿”å›:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        from osgeo import gdal
        
        logging.info(f"ä½¿ç”¨åˆ†æ‰¹é•¶åµŒç­–ç•¥ï¼Œæ¯æ‰¹å¤„ç† {batch_size} ä¸ªæ–‡ä»¶")
        logging.info(f"æ€»æ–‡ä»¶æ•°: {len(input_files)}ï¼Œé¢„è®¡åˆ† {(len(input_files) + batch_size - 1) // batch_size} æ‰¹å¤„ç†")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾ä¸­é—´ç»“æœ
        temp_dir = os.path.join(os.path.dirname(output_path), "temp_batch")
        os.makedirs(temp_dir, exist_ok=True)
        
        batch_files = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(input_files), batch_size):
            batch = input_files[i:i + batch_size]
            batch_num = i // batch_size + 1
            batch_output = os.path.join(temp_dir, f"batch_{batch_num:03d}.tif")
            
            logging.info(f"å¤„ç†ç¬¬ {batch_num} æ‰¹ï¼ŒåŒ…å« {len(batch)} ä¸ªæ–‡ä»¶")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            success = create_unified_mosaic(batch, batch_output)
            if not success:
                logging.error(f"ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥")
                return False
            
            batch_files.append(batch_output)
            logging.info(f"ç¬¬ {batch_num} æ‰¹å¤„ç†å®Œæˆ")
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        logging.info("å¼€å§‹åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ...")
        final_success = create_unified_mosaic(batch_files, output_path)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        logging.info("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        cleaned_count = 0
        for batch_file in batch_files:
            try:
                if os.path.exists(batch_file):
                    os.remove(batch_file)
                    cleaned_count += 1
                    logging.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {batch_file}")
            except Exception as e:
                logging.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {batch_file}: {str(e)}")
        
        logging.info(f"å·²æ¸…ç† {cleaned_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
        
        # å¼ºåˆ¶åˆ é™¤ä¸´æ—¶ç›®å½•ï¼ˆä½¿ç”¨æ–°çš„å¼ºåˆ¶åˆ é™¤å‡½æ•°ï¼‰
        temp_dir_removed = force_remove_directory(temp_dir)
        if not temp_dir_removed:
            logging.warning(f"ä¸´æ—¶ç›®å½•åˆ é™¤å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»è¦åŠŸèƒ½: {temp_dir}")
        
        return final_success
        
    except Exception as e:
        logging.error(f"åˆ†æ‰¹é•¶åµŒæ—¶å‡ºé”™: {str(e)}")
        
        # å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            logging.info("å‡ºé”™æ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            if 'batch_files' in locals():
                for batch_file in batch_files:
                    try:
                        if os.path.exists(batch_file):
                            os.remove(batch_file)
                            logging.debug(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {batch_file}")
                    except Exception as cleanup_e:
                        logging.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {batch_file}: {str(cleanup_e)}")
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if 'temp_dir' in locals():
                force_remove_directory(temp_dir)
                
        except Exception as cleanup_e:
            logging.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {str(cleanup_e)}")
        
        return False

def main():
    """
    ä¸»å‡½æ•° - æŒ‰åŒºåŸŸå’Œå¹´ä»½åˆ†ç»„è¿›è¡Œé•¶åµŒ
    """
    # å®šä¹‰è·¯å¾„
    input_dir = r"D:\åœ°ç†æ‰€\è®ºæ–‡\ä¸œå—äºš10mäººå·¥æ—æå–\æ•°æ®\æ­£å¼åˆ†ç±»_10.29\1.GEEå¯¼å‡ºç»“æœ\4"
    output_dir = r"D:\åœ°ç†æ‰€\è®ºæ–‡\ä¸œå—äºš10mäººå·¥æ—æå–\æ•°æ®\æ­£å¼åˆ†ç±»_10.29\2.GEEå¯¼å‡ºç»“æœ_ç»“æœåˆå¹¶"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_path = setup_logging(output_dir)
    logging.info(f"å¼€å§‹æŒ‰åŒºåŸŸå’Œå¹´ä»½åˆ†ç»„çš„é•¶åµŒå¤„ç†")
    logging.info(f"è¾“å…¥ç›®å½•: {input_dir}")
    logging.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logging.info(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
    
    # è®°å½•ç³»ç»Ÿä¿¡æ¯å’Œæ€§èƒ½é…ç½®
    import psutil
    cpu_count = psutil.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    logging.info(f"ç³»ç»Ÿä¿¡æ¯: {cpu_count} ä¸ªCPUæ ¸å¿ƒ, {memory_gb:.1f}GB å†…å­˜")
    logging.info("æ€§èƒ½é…ç½®: é«˜æ€§èƒ½æ¨¡å¼ - 4GB GDALç¼“å­˜, 2GB Warpå†…å­˜, ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ")
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_dir):
        logging.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    
    try:
        # æŒ‰åŒºåŸŸå’Œå¹´ä»½åˆ†ç»„æ–‡ä»¶
        logging.info("="*50)
        logging.info("ç¬¬ä¸€æ­¥ï¼šæŒ‰åŒºåŸŸå’Œå¹´ä»½åˆ†ç»„æ–‡ä»¶")
        grouped_files = group_files_by_zone_year(input_dir)
        
        if not grouped_files:
            logging.error("æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„æ–‡ä»¶")
            return
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_groups = len(grouped_files)
        total_files = sum(len(files) for files in grouped_files.values())
        logging.info(f"æ€»å…±æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œåˆ†ä¸º {total_groups} ä¸ªåŒºåŸŸ-å¹´ä»½ç»„åˆ")
        
        # é€ä¸ªå¤„ç†æ¯ä¸ªåŒºåŸŸ-å¹´ä»½ç»„åˆ
        logging.info("="*50)
        logging.info("ç¬¬äºŒæ­¥ï¼šå¼€å§‹é€ä¸ªé•¶åµŒå¤„ç†")
        
        success_count = 0
        failed_groups = []
        
        for i, ((zone, year), files) in enumerate(grouped_files.items(), 1):
            group_start_time = time.time()
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_filename = f"{zone}_{year}.tif"
            output_path = os.path.join(output_dir, output_filename)
            
            logging.info(f"å¤„ç†ç¬¬ {i}/{total_groups} ç»„: {zone}_{year}")
            logging.info(f"  åŒ…å«æ–‡ä»¶æ•°: {len(files)}")
            logging.info(f"  è¾“å‡ºæ–‡ä»¶: {output_filename}")
            
            # è®¡ç®—å½“å‰ç»„æ–‡ä»¶æ€»å¤§å°
            try:
                group_size_gb = sum(os.path.getsize(f) for f in files) / (1024**3)
                logging.info(f"  æ•°æ®å¤§å°: {group_size_gb:.2f} GB")
            except Exception as e:
                logging.warning(f"  æ— æ³•è®¡ç®—æ–‡ä»¶å¤§å°: {str(e)}")
                group_size_gb = 0
            
            # é€‰æ‹©é•¶åµŒç­–ç•¥
            if len(files) > 10:
                logging.info(f"  ä½¿ç”¨åˆ†æ‰¹é•¶åµŒç­–ç•¥ï¼ˆæ–‡ä»¶æ•°é‡: {len(files)}ï¼‰")
                success = create_batch_mosaic(files, output_path, batch_size=8)
            else:
                logging.info(f"  ä½¿ç”¨ç›´æ¥é•¶åµŒç­–ç•¥ï¼ˆæ–‡ä»¶æ•°é‡: {len(files)}ï¼‰")
                success = create_unified_mosaic(files, output_path)
            
            group_end_time = time.time()
            group_time = group_end_time - group_start_time
            
            if success:
                success_count += 1
                # è·å–è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
                if os.path.exists(output_path):
                    file_size_gb = os.path.getsize(output_path) / (1024**3)
                    processing_speed = file_size_gb / group_time * 60 if group_time > 0 else 0
                    logging.info(f"  âœ“ é•¶åµŒæˆåŠŸï¼")
                    logging.info(f"    è¾“å‡ºå¤§å°: {file_size_gb:.2f} GB")
                    logging.info(f"    å¤„ç†æ—¶é—´: {group_time:.2f} ç§’")
                    logging.info(f"    å¤„ç†é€Ÿåº¦: {processing_speed:.2f} GB/åˆ†é’Ÿ")
                else:
                    logging.error(f"  âœ— é•¶åµŒå¤±è´¥ï¼šè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                    failed_groups.append(f"{zone}_{year}")
            else:
                logging.error(f"  âœ— é•¶åµŒå¤±è´¥ï¼š{zone}_{year}")
                failed_groups.append(f"{zone}_{year}")
            
            logging.info("-" * 30)
        
        # å¤„ç†å®Œæˆç»Ÿè®¡
        end_time = time.time()
        total_time = end_time - start_time
        
        logging.info("="*50)
        logging.info("é•¶åµŒå¤„ç†å®Œæˆç»Ÿè®¡:")
        logging.info(f"æ€»å¤„ç†ç»„æ•°: {total_groups}")
        logging.info(f"æˆåŠŸç»„æ•°: {success_count}")
        logging.info(f"å¤±è´¥ç»„æ•°: {len(failed_groups)}")
        logging.info(f"æˆåŠŸç‡: {success_count/total_groups*100:.1f}%")
        logging.info(f"æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
        
        if failed_groups:
            logging.error("å¤±è´¥çš„ç»„åˆ:")
            for group in failed_groups:
                logging.error(f"  - {group}")
        
        # è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
        logging.info("\nç”Ÿæˆçš„é•¶åµŒæ–‡ä»¶:")
        for file in os.listdir(output_dir):
            if file.endswith('.tif'):
                file_path = os.path.join(output_dir, file)
                file_size_gb = os.path.getsize(file_path) / (1024**3)
                logging.info(f"  {file} ({file_size_gb:.2f} GB)")
        
        if success_count == total_groups:
            logging.info("ğŸ‰ æ‰€æœ‰åŒºåŸŸ-å¹´ä»½ç»„åˆé•¶åµŒå¤„ç†æˆåŠŸå®Œæˆ!")
        else:
            logging.warning(f"âš ï¸  éƒ¨åˆ†ç»„åˆå¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„ {len(failed_groups)} ä¸ªç»„åˆ")
            
    except Exception as e:
        logging.error(f"é•¶åµŒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        logging.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    logging.info("="*50)

if __name__ == "__main__":
    main()